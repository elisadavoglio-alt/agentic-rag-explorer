A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions
Agada Joseph Oche
Corresponding author: joe88data1@gmail.com Bredesen Center for Interdisciplinary Research, University of Tennessee, Knoxville, USA, 37996
Ademola Glory Folashade
gloryademola112@gmail.com Bredesen Center for Interdisciplinary Research, University of Tennessee, Knoxville, USA, 37996
Tirthankar Ghosal
National Center for Computational Sciences, Oak Ridge National Laboratory, Oak Ridge, USA, 37830
Arpan Biswas
University of Tennessee-Oak Ridge Innovation Institute, University of Tennessee, Knoxville, USA, 37996
(July 25, 2025)

Abstract
Retrieval-Augmented Generation (RAG) represents a major advancement in natural language processing (NLP), combining large language models (LLMs) with information retrieval systems to enhance factual grounding, accuracy, and contextual relevance. This paper presents a comprehensive systematic review of RAG, tracing its evolution from early developments in open-domain question answering to recent state-of-the-art implementations across diverse applications. The review begins by outlining the motivations behind RAG, particularly its ability to mitigate hallucinations and outdated knowledge in parametric models. Core technical components‚Äîretrieval mechanisms, sequence-to-sequence generation models, and fusion strategies‚Äîare examined in detail. A year-by-year analysis highlights key milestones and research trends, providing insight into RAG‚Äôs rapid growth. The paper further explores the deployment of RAG in enterprise systems, addressing practical challenges related to retrieval of proprietary data, security, and scalability. A comparative evaluation of RAG implementations is conducted, benchmarking performance on retrieval accuracy, generation fluency, latency, and computational efficiency. Persistent challenges such as retrieval quality, privacy concerns, and integration overhead are critically assessed. Finally, the review highlights emerging solutions, including hybrid retrieval approaches, privacy-preserving techniques, optimized fusion strategies, and agentic RAG architectures. These innovations point toward a future of more reliable, efficient, and context-aware knowledge-intensive NLP systems.

Keywords: Retrieval Augmented Generation (RAG), Large Language Model (LLM), Generative AI, Natural Language Model (NLP)

1 Introduction
Since its formal introduction in the seminal work of (Lewis2020,) in 2020, Retrieval-Augmented Generation (RAG) has witnessed rapid advancements, marked by a significant surge in research interest and scholarly publications. This paper offers a unique and comprehensive review of the key developments and contributions in the field to date. The remainder of this introduction outlines the background and motivation for this review, defines its scope and objectives, and provides an overview of the paper‚Äôs organization.

1.1 Background and Motivation
Large-scale pre-trained language models have demonstrated an ability to store vast amounts of factual knowledge in their parameters, but they struggle with accessing up-to-date information and providing verifiable sources. This limitation has motivated techniques that augment generative models with information retrieval. Retrieval-Augmented Generation (RAG) emerged as a solution to this problem, combining a neural retriever with a sequence-to-sequence generator to ground outputs in external documents (Lewis2020,). The seminal work of (Lewis2020,) introduced RAG for knowledge-intensive tasks, showing that a generative model (built on a BART encoder‚Äìdecoder) could retrieve relevant Wikipedia passages and incorporate them into its responses, thereby achieving state-of-the-art performance on open-domain question answering. RAG is built upon prior efforts in which retrieval was used to enhance question answering and language modeling (Lee2019,; Guu2020,; Karpukhin2020,). Unlike earlier extractive approaches, RAG produces free-form answers while still leveraging non-parametric memory, offering the best of both worlds: improved factual accuracy and the ability to cite sources. This capability is especially important to mitigate hallucinations (i.e., believable but incorrect outputs) and to allow knowledge updates without retraining the model (Lewis2020,; IBM2023,).
Since its introduction, RAG has gained significant attention in both research and industry. A growing body of literature has extended RAG with improved retrievers and generators, and the approach has been applied to a wide range of domains. By 2023, the RAG paradigm underpinned hundreds of research publications and numerous commercial systems (Gao2024RAGSurvey,; Merritt2023,). In academia, researchers have scaled up retrieval-augmented models and refined their architectures‚Äîexamples include leveraging larger pre-trained models with retrieval in the loop (e.g., Izacard2022,; Borgeaud2022,). In parallel, industry adoption of RAG has been swift: leading tech companies have integrated retrieval-augmented generators into search engines, virtual assistants, and enterprise question-answering applications (IBM2023,; Merritt2023,). RAG now powers applications from open-domain QA and customer support chatbots to tools that automatically generate answers with supporting evidence. This broad adoption underscores the significance of RAG as a foundation for making generative AI more reliable and knowledge-aware. This paper provides a unique perspective on to review of literature in RAG by providing detailed yearly research progress in RAG, developing new perspectives, and evaluating trends.

1.2 Scope and Objectives
The objective of this systematic review is to provide a comprehensive overview of the development of RAG and its expanding role in information access. We aim to answer several key research questions: (1) How has RAG been progressed every year since its inception, and what are the major technical milestones in its research and deployment? (2) What challenges and solutions have emerged for integrating RAG with proprietary or private data sources, and what gaps remain (e.g., in security and privacy)? (3) How has RAG been used in accelerating material discovery and characterization (4) How are RAG systems categorized and how does this categorization affect their performance?. By addressing these questions, the review seeks to chart the evolution of RAG, evaluate its current capabilities and limitations, and identify areas for future work.
Over the past few years, progress in RAG has been marked by continuous innovation and new applications. We chronicle the advancement year-by-year, highlighting important academic contributions and industry developments that have shaped the field. Special attention is given to the integration of RAG with proprietary data‚Äîan area of growing interest as organizations apply RAG to internal knowledge bases. This involves examining techniques for efficient retrieval on private corpora and the handling of sensitive information, as well as open issues around data privacy (Zeng2024,). Recent systems have also demonstrated that users can interact with an RAG-powered agent to obtain information directly from the web or a document corpus, rather than through traditional ranked search results (Nakano2022,; Shuster2022,). This paradigm blurs the line between search engine and dialogue agent, opening questions about usability, accuracy, and trust in such interfaces. Overall, the review considers this and also consolidates knowledge on how RAG techniques have matured and what objectives remain for future research and development.

1.3 Paper Organization
The remainder of this paper is structured as follows. Section 2 (Methodology) explains the review methodology, including the literature search strategy, inclusion/exclusion criteria, and approach to data synthesis. Section 3 (Foundations of RAG) provides a technical overview of retrieval-augmented generation, describing its core components (retrievers, indexes, generators) and the baseline architectures introduced by seminal works. Section 4 (Year-by-Year Progress) presents a chronological synthesis of RAG developments from 2017 onward, highlighting key research milestones. Section 5 (RAG for Proprietary data and Industry Implementation) examines enterprise implementation of RAG on proprietary data by key industry players. Section 6 (RAG Systems Evaluation) benchmarks different RAG implementations and variants, summarizing their performance across standard datasets and tasks. Section 7 (Challenges of RAG Systems), Section 8 (Discussion and Future Direction) and Section 9 (Conclusion) finally provide current research gaps and potential future directions to expand the applications to various domain problems.

2 Methodology
This section details the systematic review methodology employed to survey RAG papers. It comprises three main steps: (1) designing a Search Strategy to capture a wide range of relevant works, (2) defining Inclusion and Exclusion Criteria to refine the initial corpus, and (3) implementing a Data Extraction and Synthesis process to analyze and consolidate findings.

2.1 Search Strategy
To ensure comprehensive coverage, we searched both academic and industry-focused literature on RAG. Multiple digital libraries were queried, including ACL Anthology, IEEE Xplore, ACM Digital Library, and Google Scholar. We included documents published from 2017 up to the end of mid 2025, covering early ‚Äúretrieve-and-generate‚Äù approaches and more recent RAG-specific techniques.
Keywords and Databases.
We used a set of pre-defined keywords, such as ‚Äúretrieval-augmented generation (RAG),‚Äù ‚Äúdense retrieval,‚Äù ‚Äúhybrid retrieval LLM,‚Äù ‚ÄúRAG proprietary data,‚Äù and ‚ÄúLLM web search.‚Äù These queries captured works ranging from open-domain QA to secure enterprise implementations. Each keyword search was executed on the above-listed databases, resulting in a pool of references that included journal articles, conference papers, technical reports, and white papers.
Initial Screening.
A comprehensive list of potentially relevant works was formed by merging all search results and removing duplicates. Abstracts and titles were checked to confirm alignment with the RAG focus. If a work concentrated solely on retrieval or generation in isolation, without discussing how these components integrate, it was set aside for possible exclusion.

2.2 Inclusion and Exclusion Criteria
We next applied a formal screening process to determine which references genuinely contributed insights into RAG. The criteria below guided our decisions:
2.2.1 Inclusion Criteria
‚Ä¢ Relevant to RAG or closely related baselines: Works that clearly integrated a retriever with a generative language model or used retrieval to supply context to a generator (Lewis2020,).
‚Ä¢ Knowledge-Intensive Tasks: Studies centered on open-domain QA, fact-checking, knowledge-grounded dialogue, or other tasks benefiting from external document retrieval.
‚Ä¢ Peer-Reviewed or Reputable Sources: Publications presented at major AI/NLP venues (ACL, NeurIPS, ICML, EMNLP) or recognized industrial R&D labs (e.g., IBM, Meta, NVIDIA) (IBM2023,; Merritt2023,).
‚Ä¢ Preprint: Preprints are also included to broaden the scope of the survey.
‚Ä¢ English Language: For consistency and to support thorough evaluation, only English texts were included. end
2.2.2 Exclusion Criteria
‚Ä¢ Solely Retrieval or Solely Generation: Articles focusing strictly on IR techniques or purely generative models without explicit retrieval-augmented integration were not included.
‚Ä¢ Minimal Discussion of RAG: Any mention of retrieval+generation was peripheral or superficial, lacking substantial results or analyses.
‚Ä¢ Non-Substantive Publications: Very short abstracts, publicity notes, or materials without verifiable methodology were excluded.
‚Ä¢ Non-English Papers: Not considered due to feasibility constraints.
Based on these criteria, the initial corpus was refined into a finalized set of documents deemed pertinent to the state-of-the-art in RAG.

2.3 Data Extraction
For each included publication, we collected key information, such as basic bibliographic details, the retrieval method (e.g., dense vs. sparse), the generator architecture (e.g., T5, BART, GPT), and the evaluated tasks or datasets. This allowed us to systematically compare different RAG implementations and their reported performance. We also looked for and extracted information on the challenges facing RAG implementation. The survey is not limited to peer-reviewed journal articles and conference proceedings, preprints, technical reports, and industry white papers were also reviewed. The review covers the application of RAG systems in all domains
Synthesis Process.
All extracted details/data were gathered in a central repository, allowing cross-study comparisons. We grouped research outputs by year of publication to track the chronological evolution of RAG, highlighting seminal breakthroughs and subsequent expansions. In line with systematic review principles, we combined both qualitative (themes, research directions) and quantitative (performance figures, latency measures) observations.
Ensuring Reliability.
Disagreements during the review were resolved through discussion or by consulting a third reviewer. This final step ensured consistent application of the inclusion/exclusion criteria and reliable data extraction. The data collected then served as the foundation for our analysis in subsequent sections, including discussions on year-by-year progress, enterprise applications,and proposed solutions.

3 Foundations of RAG
3.1 Definition and Key Concepts
Retrieval-Augmented Generation (RAG)
: RAG is a framework that combines a neural text retrieval module with a text generation module to improve the quality of generated responses in knowledge-intensive tasks. Formally, a RAG model augments a sequence-to-sequence (seq2seq) generator with access to an external text corpus (non-parametric memory) via a retriever (Lewis2020,; Karpukhin2020,). Given an input query x, the retriever R selects a small subset of relevant documents Z={z1,z2,‚Ä¶,zK} from a large corpus ùíû (with K‚â™|ùíû|) (Karpukhin2020,). The generator then conditions on both the query x and the retrieved documents Z to produce an output y (such as an answer or a descriptive text). Formally, the RAG model can be viewed as a latent variable generative model that defines a probability distribution over outputs y by marginalizing over the retrieved documents zi:


P(y‚à£x)=‚àëi=1KPret(zi‚à£x)Pgen(y‚à£x,zi),


(1)

where Pret(zi‚à£x) is the probability of retrieving document zi given query x (the retriever‚Äôs output distribution), and Pgen(y‚à£x,zi) is the generator‚Äôs conditional probability of producing y given x and a particular retrieved document zi. In practice, Pret(zi‚à£x) is typically non-zero only for the top-K retrieved items, providing a tractable approximation to the full sum over the corpus (Lewis2020,). The retriever R itself can be defined as a function R(x,ùíû)‚ÜíZ that takes a query and returns a small subset Z of corpus ùíû (with |Z|=K‚â™|ùíû|) likely to contain information relevant to x (Karpukhin2020,). By design, RAG models maintain two kinds of memory: a parametric memory (the knowledge encoded in the generator‚Äôs weights) and a non-parametric memory (the external text corpus accessed via retrieval) (Lewis2020,). A standard RAG architecture is illustrated in Figure 1 below. A key distinction between RAG and pure large language model (LLM) generation is the use of this external non-parametric knowledge source at inference time. Traditional LLM-based generation relies solely on the model‚Äôs internal parameters for knowledge, which can lead to hallucinations and factual inaccuracies when the model‚Äôs training data does not adequately cover the query‚Äôs topic (Lewis2020,). In contrast, RAG explicitly grounds the generation of retrieved documents that serve as up-to-date evidence, enabling the model to generate content supported by those documents. This retrieval step means that RAG‚Äôs outputs can be more accurate and factually correct compared to generation from a standalone LLM, especially for knowledge-intensive queries. Empirically, (Lewis2020,) demonstrates that a RAG model generates more specific and factual responses than a parametric-only generator, since the retrieved text provides verified information that the generator can incorporate. Another benefit is that the knowledge in a RAG system can be easily updated by modifying the document index (or corpus) without retraining the generator, addressing the stiffness of LLMs that have fixed knowledge up to their training cutoff date. In summary, RAG introduces a modular architecture where a retrieval component supplies relevant context ‚Äújust in time‚Äù for the generator, marrying the strengths of Information Retrieval (IR) with those of large-scale generation.
Figure 1:Illustration of a RAG Architecture.
Chunking, Embedding, and (Re)ranking
:A typical RAG pipeline consists of four stages: chunking, embedding, (re)ranking, and generation. First, chunking is applied to the knowledge source: large documents are segmented into smaller, self-contained pieces (e.g., paragraphs or passages) for indexing. Using fine-grained text chunks as retrieval units improves the chance that a query will surface a highly relevant fragment, rather than an entire lengthy document (Lewis2020,). For example, open-domain QA systems split Wikipedia articles into passage chunks to enable pinpoint retrieval of answer-containing segments (Karpukhin2020,; Lewis2020,). The chunk size is typically tuned to balance context completeness and specificity ‚Äì chunks must be large enough to contain useful context, yet small enough to match queries narrowly and fit within model context windows. Next, each chunk is embedded into a high-dimensional vector representation that encodes its semantic content. This is usually done with a transformer-based bi-encoder that produces dense vector embeddings of text (Karpukhin2020,). The embeddings serve as keys in a vector index (or vector database) that supports efficient nearest-neighbor search. At query time, the user‚Äôs query is likewise embedded into the same vector space, and the system performs similarity search to retrieve the most relevant chunk vectors. In contrast to sparse keyword search, dense embeddings enable semantic matching: a query about ‚Äúfinancial earnings‚Äù can retrieve a chunk about ‚Äúquarterly revenue‚Äù even if exact words differ (Karpukhin2020,). Modern RAG implementations often combine dense retrieval with lightweight filtering or hybrid search (e.g., BM25 + embeddings) to improve recall for difficult queries. The result of the initial retrieval stage is a candidate set of top-k chunks that are potentially relevant to the query. To further improve precision, an optional re-ranking step is applied on the retrieved candidates before generation. The top-k chunks from the first stage may contain some irrelevant or only tangentially related items, since embedding similarity is a coarse proxy for relevance. A re-ranker model (typically a cross-encoder transformer that jointly encodes query and document) evaluates each retrieved chunk in the context of the query and produces a refined relevance score (Nogueira2019,). By re-scoring and sorting the candidates, the re-ranker ensures that the most pertinent chunks (for example, those actually containing the answer to a question) are ranked highest. This two-stage retrieval process ‚Äì a fast dense retriever followed by a more accurate but expensive re-ranker ‚Äì has been shown to significantly boost retrieval performance on knowledge-intensive benchmarks. For instance, neural cross-attention re-rankers achieve substantially higher accuracy than single-stage retrievers alone (Nogueira2019,). In practice, re-ranking is crucial in high-stakes applications (e.g., legal or medical QA), where one must maximize the likelihood that the top context passages truly address the user‚Äôs query. After re-ranking, the top N (e.g. 3‚Äì5) chunks are selected as the final context passages for the generative model. In the generation stage, the LLM produces an answer or response conditioned on the retrieved external chunks. Typically, a sequence-to-sequence model (such as T5 or BART) is used so that the retrieved text can be prepended or incorporated into the model‚Äôs input along with the user query (Lewis2020,; Raffel2020,). During training, the model learns to copy or attend to the relevant facts from the retrieved documents and integrate them into a coherent output. This approach allows the generator to cite up-to-date, specific information beyond its parametric knowledge. For example, (Lewis2020,) show that a RAG model (BART-based) can accurately answer open-domain questions by retrieving and conditioning on Wikipedia text, dramatically reducing hallucinations compared to a standalone LLM. The generated output can also include references to source documents, providing traceability for the facts used. Retrieval augmentation thus serves as a ‚Äúlive memory‚Äù for the LLM: it supplies factual grounding from an external knowledge base while the language model creates fluent and contextually relevant text. Notably, recent large-scale studies have demonstrated that even very large models benefit from retrieval augmentation. For instance, the RETRO model augments a 7.5-billion-parameter transformer with a database of trillions of tokens, yielding improved perplexity and factual accuracy by looking up passages during generation (Borgeaud2022,). In summary, chunking, embedding, re-ranking, and generation work in concert in RAG systems to leverage external knowledge ‚Äì the retrieval components identify and prioritize relevant information, and the generation component uses that information to produce answers that are both informative and grounded in source data. This modular design has become a foundation for building more reliable and explainable AI assistants in knowledge-intensive domains.
3.2 Technical Components of RAG
A RAG system is composed of two primary components ‚Äì a retriever module and a generator module ‚Äì along with a strategy for fusing their outputs. We break down these components and the underlying mechanics as follows.
Retriever Module (Dense Passage Retrieval).
The retriever‚Äôs job is to efficiently identify which pieces of text in a large corpus are relevant to the input query x. Modern RAG implementations typically use dense retrievers like Dense Passage Retrieval (DPR) (Karpukhin2020,) in lieu of traditional keyword search. In DPR, a bi-encoder architecture is employed: a question encoder Eq(x) maps the query x to a d-dimensional vector, and a passage encoder Ep(d) maps each candidate document (or passage) d in the corpus to a d-dimensional vector in the same space. Relevance is assessed via a similarity score, usually the dot product of these vectors:


sim(x,d)=Eq(x)‚ä§Ep(d).



At query time, the retriever computes vq=Eq(x) and then finds the top-K documents whose vector Ep(d) has highest inner product with vq. This search for maximum inner product can be implemented efficiently using Approximate Nearest Neighbor techniques (e.g., FAISS) to handle millions of documents in sub-linear time. The output of the retriever is the set Z={z1,‚Ä¶,zK} of top-ranked documents and potentially their similarity scores. One can view the retriever as defining a probability distribution Pret(z‚à£x) over documents z in the corpus, such that:


Pret(z‚à£x)‚àùexp(Eq(x)‚ä§Ep(z)),


(2)

with the normalization ‚àëd‚ààùíûexp(Eq(x)‚ä§Ep(d)) (in practice approximated by summing over retrieved candidates rather than all of ùíû). In other words, the retriever assigns higher probability (or score) to documents whose embedding is most similar to the query‚Äôs embedding. The DPR retriever is usually first trained on pairs of questions and relevant passages to ensure Eq and Ep produce representations that maximize dot-products for true Q‚ÄìA pairs and minimize them for irrelevant pairs. The training objective is often a contrastive loss: for a given question q with a gold relevant passage p+ and a set of negative passages {p1‚àí,‚Ä¶,pN‚àí}, the encoder is trained to maximize sim(q,p+) while minimizing sim(q,p‚àí) for negatives. This can be formulated as a cross-entropy loss treating the positive passage as the correct class among one positive and N negatives:


‚Ñíret(q,p+)=‚àílogexp(sim(q,p+))exp(sim(q,p+))+‚àëj=1Nexp(sim(q,pj‚àí))



which encourages Eq and Ep to embed true pairs closer together than any negative pair (Karpukhin2020,). After training, the retriever can generalize to new queries: it embeds the query and efficiently finds the nearest neighbor passages in the index. This retrieval step is crucial because it narrows down the evidence from potentially billions of tokens to a manageable subset that the generator will actually consider.
Generator Module (Conditional Seq2Seq Model).
The generator in a RAG pipeline is typically a sequence-to-sequence language model that produces the final answer or output text, given the input query and retrieved documents. Formally, the generator defines a conditional distribution Pgen(y‚à£x,Z) over output sequences, where Z={z1,‚Ä¶,zK} are the retrieved passages. The generator is often initialized from a pre-trained transformer-based seq2seq model (such as BART or T5) to leverage rich language generation capabilities (Lewis2020,). During generation, the model is provided with the question (or prompt) as well as the content of the retrieved documents. There are multiple ways to feed the retrieved context to the generator:
‚Ä¢ In an early fusion approach, one can concatenate the query x with the text of all retrieved passages into a single extended input sequence (perhaps with special separators) and have the seq2seq model attend to all of it at once. This is straightforward: the generator effectively treats the combined text as context and learns to pick out the relevant bits when producing the answer. However, this can be difficult if K is large, as the input may become very long.
‚Ä¢ In the late fusion approach adopted by RAG (Lewis2020,), the generator considers one retrieved document at a time as a context and then marginalizes over the document choices (as in Eq. 1). Specifically, the RAG-Sequence variant fixes a single document zi as context for generating the entire output and computes P(y‚à£x) by summing the probabilities P(y‚à£x,zi) weighted by the retriever‚Äôs confidence P(zi‚à£x). Another variant, RAG-Token, allows the generator to switch between documents at the token level, effectively marginalizing over the document choice for each generated token (Lewis2020,). In both cases, the generator Pgen(y‚à£x,z) itself works like a standard seq2seq model: it factorizes over the output tokens y1,‚Ä¶,yT as ‚àèt=1TPŒ∏(yt‚à£x,z,y<t), i.e. it generates one token at a time, attending to the input query x and the content of z (or multiple z‚Äôs if early fusion). The generator‚Äôs architecture typically uses an encoder-decoder Transformer: the encoder encodes the combination of x and z, and the decoder produces y autoregressively.
Because the generator is conditioning on retrieved evidence, it tends to produce outputs that are supported by that evidence. For example, if the query asks for a specific factual answer, the generator can copy or rephrase the needed information from one of the retrieved passages. This is in contrast to a vanilla language model which would attempt to rely on parametric knowledge (which might be outdated or incomplete). In sum, the generator module in RAG is responsible for fluent and coherent text generation, but crucially it is grounded by the retrieval context, which guides it toward accurate content.
Fusion Mechanisms and Answer Aggregation.
A critical aspect of RAG systems is how to fuse information from multiple retrieved documents z1,‚Ä¶,zK when producing the final answer. Different fusion strategies have been explored: - Marginalization (Probabilistic Fusion): As described, RAG treats the retrieved documents as latent variables and marginalizes over them (Lewis2020,). This means the model doesn‚Äôt commit to one retrieved source up front; instead, it considers each in turn and combines their contributions by summing probabilities. Concretely, if y is an output sequence, a RAG model might compute its probability by P(y|x)=‚àëi=1KPret(zi|x)Pgen(y|x,zi) (Eq. 1). During training, this encourages the model to distribute probability mass across any document that could yield the correct answer, reinforcing multiple evidence paths. At inference, one can approximately marginalize by taking the most likely y under this mixture model. - Direct concatenation (Early Fusion): As mentioned, another approach is to simply feed all top-K retrieved texts into the generator at once (often referred to as ‚ÄúFusion-in-Decoder‚Äù when implemented in a decoder-attention context). In this setup, the generator effectively performs its own internal fusion by attending over a combined context. This approach has the advantage that the generator can directly cross-attend to multiple documents and integrate their content, but it may require a more powerful model to handle very long concatenated inputs. It also does not explicitly model the per-document probabilities P(zi|x). - Weighted Aggregation: Some systems introduce an attention or weighting mechanism over retrieved documents. For instance, the generator‚Äôs decoder might assign different attention weights to different passages at each decoding step, effectively learning which source is most useful for generating the next token. This can be seen as a soft fusion: rather than hard marginalization or simple concatenation, the model dynamically blends information. In practice, approaches like (Lewis2020,) found that marginalization (which is a form of weighting by the retriever‚Äôs scores) works well, especially when the retriever is accurate. Other works have since experimented with learnable fusion weights or iterative retrieval-generation cycles, but the core idea is the same: the model must reconcile possibly conflicting or complementary information from multiple documents to produce a single, coherent answer.
The choice of fusion affects the system‚Äôs ability to handle conflicting evidence and the credit assignment during training (i.e., which document gets ‚Äúcredit‚Äù for a correct answer). RAG‚Äôs probabilistic fusion provides a principled way to train the retriever and generator together by marginalizing, whereas direct concatenation treats the problem in a single forward pass of a generator (often fine for tasks where evidence is mostly additive or when using very large generators). Fusion strategies continue to be an active area of research, but they all serve the goal of effectively utilizing multiple retrieved pieces of text to improve answer completeness and correctness.
Training and Optimization.
Training a RAG model involves objectives for both the retriever and the generator, which can be combined in an end-to-end manner. A common training approach is as follows: first, pre-train or initialize the retriever on a relevance task and initialize the generator on a language modeling or seq2seq task (often using a pre-trained model checkpoint). Then, perform joint fine-tuning on the target task (e.g., a QA dataset or a knowledge-intensive dialogue dataset) by maximizing the likelihood of the correct output y‚àó given the input x and allowing gradients to flow into both the generator and retriever. The training objective for the whole RAG system can be written as the expected negative log-likelihood:


‚ÑíRAG=‚àílogP(y‚àó‚à£x),



where P(y‚àó‚à£x) is computed as in Eq. 1. Because P(y‚àó|x) is a sum over documents, the gradient will encourage whichever retrieved documents zi that helped predict y‚àó (by giving high Pgen(y‚àó|x,zi)) to have their retrieval probability Pret(zi|x) increased. In effect, the model learns to adjust the retriever to fetch better supporting documents and adjust the generator to rely on them appropriately. This joint training is typically done with standard backpropagation; since the retriever‚Äôs selection operation is not differentiable for all documents, one uses the top-K approximation (only those contribute to the loss) and treats the retrieval probabilities for those as soft variables. (Lewis2020,) report that initializing the retriever with DPR and then fine-tuning end-to-end yields the best results, as opposed to training from scratch. Notably, the retriever is trained indirectly here: it does not receive explicit labels of which document is correct, but the generator‚Äôs success or failure on producing y‚àó provides a supervision signal. This is sometimes called ‚Äúself-supervised‚Äù retriever training or ‚Äúfeedback‚Äù training.
In addition to end-to-end training, various optimization tricks may be used: e.g., using a small learning rate for the retriever if it‚Äôs already strong, or alternating between retriever-focused and generator-focused updates. In some cases, researchers have also explored contrastive learning at the generation level (to reduce ambiguity between retrieved passages) or reward-based objectives if the task is not a straightforward next-word prediction. However, the most common training objective for RAG is the simple maximum likelihood training of the seq2seq model, augmented by the latent document marginalization. The result is a system where both components are tuned to the end task: the retriever learns to bring useful evidence, and the generator learns to incorporate that evidence into the output. This joint optimization is a major advantage of RAG over non-integrated pipelines, as it aligns the retriever‚Äôs objective with generating correct final answers (not just retrieving vaguely related documents).
3.3 Historical Context
The evolution of RAG builds upon earlier developments in open-domain question answering (QA) and neural information retrieval. Traditional open-domain QA systems were typically pipeline-based, consisting of a retrieval step followed by a reading or extraction step (Chen2017WikipediaQA,). For example, (Chen2017WikipediaQA,) introduced the DrQA system, which first used a TF-IDF or BM25 retriever to select Wikipedia articles and then fed those to a machine reader model to extract answers. This established the value of retrieving relevant text from a large corpus as an essential first step in answering open-domain questions. However, in such pipeline approaches the retriever was not integrated into the learning of the reader, and the system could not adjust retrieval based on the end task‚Äôs needs. Subsequent research sought to bridge this gap by jointly learning retrieval and answering. Notably, the concept of using learned dense representations for retrieval emerged as a powerful alternative to traditional sparse retrieval. Early milestones in this direction include latent retrieval models: (Lee2019,) proposed the ORQA model, which treats retrieval as a latent variable problem and pre-trains a neural retriever on an unsupervised ‚Äúinverse cloze task‚Äù before jointly fine-tuning it with a reader on QA. Around the same time, (Guu2020,) introduced REALM, a retrieval-augmented language model pre-training method that incorporated a differentiable retriever into a pre-training of a masked language model. REALM demonstrated that pre-training a model to retrieve and reason over Wikipedia could significantly improve open-domain QA, highlighting the benefit of coupling a language model with a learned retrieval mechanism (Guu2020,). These efforts were focused primarily on question answering (often extractive), but they laid important groundwork for retrieval-augmented generation by showing that retrieval and neural text generation can be trained in tandem.
In parallel, the idea of combining external knowledge with neural networks has roots in earlier memory-augmented models. For instance, Memory Networks (Weston2015,) and subsequent variants allowed neural networks to read from an external memory of facts and use that information to answer questions or generate responses. These models (e.g., (Weston2015,; Sukhbaatar2015,)) demonstrated the feasibility of non-parametric memory for reasoning, albeit on smaller-scale knowledge bases or synthetic tasks. While memory network architectures were often task-specific and required the memory to be relatively small or structured, they presaged the RAG approach by emphasizing that not all knowledge needs to be baked into model parameters‚Äîsome can be looked up as needed. Another line of work in dialogue systems also integrated retrieval into generation: the Wizard of Wikipedia project (Dinan2019,) is a prime example, where a conversational agent retrieves relevant Wikipedia sentences and conditions a generative dialogue model on those sentences to produce knowledgeable responses. This retrieval-based dialogue system (published in 2019) demonstrated improved factuality and depth in conversational responses, reflecting the general trend that augmenting generators with retrieved context yields more informative and correct outputs.
These developments converged in 2020 with the formalization of Retrieval-Augmented Generation by (Lewis2020,), who unified the retriever-reader architecture with seq2seq generation in an end-to-end framework. The RAG model of (Lewis2020,) was a culmination of insights from open-domain QA and neural IR: it used a dense passage retriever (Karpukhin2020,) to fetch text chunks from Wikipedia and a powerful seq2seq generator (BART) to produce answers or summaries, training both components jointly. By marginalizing over multiple retrieved documents (as in Eq. 1), the RAG system could leverage several pieces of evidence and was shown to outperform both parametric-only models and earlier retrieve-and-read pipelines on knowledge-intensive tasks like open QA (Lewis2020,). The introduction of RAG in 2020 is considered a key milestone because it generalized retrieval-augmented architectures beyond QA to any generative task requiring external knowledge. It also spurred a new line of research into knowledge-enhanced text generation, influencing subsequent models that further refined retrieval modules, document ranking, and fusion techniques for even better performance.
Execution Flow of a RAG System.
To summarize the interactions of these components, we outline the step-by-step execution flow of a typical RAG system processing a query:
1. Query Encoding: Given an input query x (e.g., a question in natural language), the retriever‚Äôs query encoder Eq first encodes x into a vector representation vq. This vector captures the semantic meaning of the query in a dense embedding space.
2. Document Retrieval: Using the query vector vq, the system performs a search over the document index (the external corpus ùíû). It computes similarity scores sim(x,d) for documents d (often via inner product with pre-computed document embeddings Ep(d)) and retrieves the top K documents with highest scores. These top-K documents Z={z1,‚Ä¶,zK} are assumed to be the most relevant pieces of text related to the query.
3. Context Preparation: The text of the retrieved documents Z is retrieved from the knowledge store. The RAG system now has access to these K passages (e.g., paragraphs from Wikipedia) that can serve as supporting context. Depending on the fusion strategy, the system either concatenates these passages or will handle them separately in the next step.
4. Answer Generation: The query x along with the retrieved context Z are fed into the generator model. If using early fusion, all of Z (or a subset, if some cutoff like K‚Ä≤ is used) is given as additional input to the encoder. If using late fusion (as in the original RAG), the generator will consider each zi in Z in turn. The generator‚Äôs decoder then produces an output sequence y (the answer or response). During this decoding, the model may attend to relevant parts of the retrieved texts. For example, the decoder might focus on a specific retrieved passage that contains a needed fact when generating the corresponding part of the answer. In RAG‚Äôs late fusion, the decoder actually generates an answer for each zi implicitly and the probabilities of tokens are combined by marginalization. In practice, one can sample or beam-search for the best output y across the combined evidence.
5. Fusion and Output: If multiple candidate outputs were considered (e.g., one per retrieved document), the model marginalizes or otherwise aggregates them to produce the final answer. Often the single most likely sequence y is selected as the output. The final answer y is then returned by the system as the response to the query. Optionally, the system might also output the passages it used (providing provenance or justification, which is a useful feature of RAG systems).
This flow involves interleaving retrieval and generation in a seamless way. Notably, steps 1‚Äì2 (retrieval) drastically reduce the problem space by focusing on a handful of documents out of potentially millions, and steps 3‚Äì5 ensure that the information in those documents is synthesized into a fluent answer. The entire process is typically very fast at inference: encoding the query and searching the index can be done in tens of milliseconds with efficient vector databases, and generation is on the order of the length of the output (with modern transformers generating dozens of tokens per second). Therefore, RAG systems can scale to handle user queries in real-time applications, all while maintaining higher accuracy by leveraging updated and explicit knowledge. The end-to-end design means that if the output is incorrect, the system can be improved by either enhancing the retriever (fetch more relevant docs) or the generator (better use of docs), or both, which aligns with the modular evaluation and training typical in IR+NLP pipeline but now integrated into a single model.
4 Year-by-Year Progress in RAG
RAG, as it is known today, was proposed in (Lewis2020,), but before then, the retrieval and read pipeline, which operates like RAG. This section reviews the evolution of RAG from the pre RAG erra till date. The anual year-by-year progress of RAG is illustrated in Figure 3.
Figure 2:Evolution of a RAG Architecture.
4.1 Initial Proposals and Early Research (2017‚Äì2019)
Before the term retrieval-augmented generation (RAG) was coined, researchers explored methods to combine information retrieval with neural models for question answering (QA) and text generation. Early open-domain QA systems typically employed a retrieve-and-read pipeline: a search module locates relevant documents, then a neural reader model extracts or generates answers (Chen2017WikipediaQA,). For instance, the 2017 DrQA framework answered questions using Wikipedia as a knowledge source by pairing a TF-IDF-based document retriever with an RNN-based reader trained to extract answer spans. Although (Chen2017WikipediaQA,) demonstrated strong performance on open-domain trivia tasks, these systems were piecemeal in nature: the retrieval and generation components were not trained jointly, and the end-to-end approach was limited to extractive answers. In 2018, work shifted toward tighter integration between retrieval and reading. (wang2018r3,) proposed R3 (Reinforced Reader-Ranker), adding a neural ranker to score retrieved passages by answer likelihood. The system then learned ranker‚Äìreader synergy via reinforcement learning, boosting open-domain QA accuracy by better filtering relevant evidence. Meanwhile, neural IR methods gained traction. (Lee2019,) introduced Latent Retrieval in their Open-Retrieval QA (ORQA) framework, training a dense retriever and a reader end-to-end with only question‚Äìanswer supervision. Specifically, the retriever embeddings were pretrained on an inverse cloze task and then adapted to select evidence documents that help the QA model answer correctly. This concept of dense retrieval, which outperformed sparse BM25 by up to 19% in exact-match QA scores, would become the foundation of subsequent RAG models. Still, these early systems were limited mostly to extractive QA, without a unified end-to-end training for generative outputs.
4.2 Major Milestones (2020‚Äì2024)
2020 ‚Äî Birth of RAG.
The year 2020 marked a turning point with the official formalization of retrieval-augmented generation. (Lewis2020,) coined the term ‚ÄúRAG‚Äù and demonstrated its power on knowledge-intensive tasks. RAG explicitly splits knowledge across (i) a neural retriever and (ii) a neural generator, each playing a distinct role. Given a query x, RAG retrieves top-k relevant passages {z1,‚Ä¶,zk} from a large text corpus (via a learned dense index) and then conditions a seq2seq model on both x and {zi}. Mathematically,


P(y‚à£x)=‚àëi=1kPŒ∏(y‚à£x,zi)Pœï(zi‚à£x),



where Pœï(zi‚à£x) is the retriever‚Äôs distribution over documents (often realized via a softmax on
Figure 3:Evolution of RAG: 2017 - Mid-2025.
inner product scores), and PŒ∏(y‚à£x,zi) is the conditional probability of the generator. (Lewis2020,) used a BART-based generator and a Dense Passage Retriever (Karpukhin2020,), outperforming older retrieve-and-read pipelines by leveraging both parametric and non-parametric memory. Simultaneously, (Guu2020,) introduced REALM: retrieval-augmented pretraining, which integrated a differentiable retriever into a language model to predict masked tokens with retrieved evidence. REALM achieved significant QA gains over conventional LMs, validating that external knowledge injection helps both at pretraining and fine-tuning. (Karpukhin2020,) also released their Dense Passage Retrieval (DPR) approach, establishing a simple but effective dual-encoder architecture. By 2020‚Äôs end, retrieval-augmented strategies had become key to state-of-the-art QA, and generative models demonstrated improved factuality by referencing retrieved text. Some particular progress made in 2020 are discussed below:
Dense neural retrieval.
Classical TF‚ÄìIDF/BM25 retrieval limited earlier pipeline QA systems. (karpukhin2020dpr,) introduced Dense Passage Retrieval (DPR), training dual BERT encoders to embed questions and passages into a shared space. DPR improved top 20 recall by 9‚Äì19 pp over BM25 and became the de facto index for RAG models.
End-to-end retriever‚Äìgenerator architectures.
Building on DPR, (lewis2020retrieval,) proposed the eponymous RAG model: a BART generator conditions on k passages retrieved (and jointly trained) via DPR. Two variants‚ÄîRAG-Sequence and RAG-Token‚Äîachieved state-of-the-art exact-match scores on Natural Questions and TriviaQA, while producing more specific, better-grounded answers than closed-book T5 of comparable size. Concurrently, the Fusion-in-Decoder (FiD) architecture of (Izacard2021,) showed that a T5 decoder attending over dozens of retrieved passages could push QA accuracy even higher, underscoring that modern seq2seq models can synthesise evidence from many documents. ... (and so on for subsequent years as in the original text)
